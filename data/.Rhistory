library(KoNLP)
library(KoNLP)
install.packages("https://cran.rstudio.com/bin/windows/contrib/3.4/KoNLP_0.80.1.zip",repos = NULL)
library(KoNLP)
## 1.1 평균과 분산 그리고 표준편차
score1 <- c(85, 90, 93, 86, 82)
score2 <- c(100, 100, 50, 0, 0)#c(85, 90, 93, 46, 42)
score3 <- c(60, 60, 50, 40, 40)#c(100, 100, 54, 50, 52)
score10 <- c(100, 50, 30, 20, 20, 20, 10, NA)
mean(score10, na.rm = T)
# 평균
mean(score1)  # [1] 87.2 : 평균값
mean(score2)  # [1] 71.2 : 평균값
mean(score3)  # [1] 71.2 : 평균값
# 중앙값(median:중위수) - 모든 데이터를 크기 순서대로 정렬시킨 후 가운데 있는 값을 의미.
# ex) 100, 100, 54, 50, 52 : 중앙값 -> 54
median(score3) # 54
# ex) 6, 6, 7, 8, 9, 10
num <- c(6, 6, 7, 8, 9, 10)
median(num) # 7.5(=(7+8)/2)
# 분산(Variance) : 편차 값을 제곱해서 마이너스 값을 플러스 값으로 바꾼 후 평균을 구하는 방법.
# ex) ((100-71.2)^2+(100-71.2)^2+(54-71.2)^2+(50-71.2)^2+(52-71.2)^2) / 5 = 554.56
score <- c(100, 100, 54, 50, 52)
mean(score) # 71.2
median(score1) # [1] 86
# 분산(Variance) : score1 <- c(85, 90, 93, 86, 82)
# ((85-86)^2+(90-86)^2+(93-86)^2+(86-86)^2+(82-86)^2)/5 = 16.4
var(score1)    # [1] 18.7
sd(score1)     # [1] 4.32435
# 분산(Variance) : 편차 값을 제곱해서 마이너스 값을 플러스 값으로 바꾼 후 평균을 구하는 방법.
# ex) ((100-71.2)^2+(100-71.2)^2+(54-71.2)^2+(50-71.2)^2+(52-71.2)^2) / 5 = 554.56
score <- c(100, 100, 54, 50, 52)
mean(score) # 71.2
# 중앙값(median:중위수) - 모든 데이터를 크기 순서대로 정렬시킨 후 가운데 있는 값을 의미.
# ex) 100, 100, 54, 50, 52 : 중앙값 -> 54
median(score3) # 54
## 1.1 평균과 분산 그리고 표준편차
score1 <- c(85, 90, 93, 86, 82)
score2 <- c(100, 100, 50, 0, 0)#c(85, 90, 93, 46, 42)
score3 <- c(60, 60, 50, 40, 40)#c(100, 100, 54, 50, 52)
score10 <- c(100, 50, 30, 20, 20, 20, 10, NA)
mean(score10, na.rm = T)
# 평균
mean(score1)  # [1] 87.2 : 평균값
mean(score2)  # [1] 71.2 : 평균값
mean(score3)  # [1] 71.2 : 평균값
## 1.1 평균과 분산 그리고 표준편차
score1 <- c(85, 90, 93, 86, 82)
score2 <- c(100, 100, 50, 0, 0)#c(85, 90, 93, 46, 42)
score3 <- c(60, 60, 50, 40, 40)#c(100, 100, 54, 50, 52)
score10 <- c(100, 50, 30, 20, 20, 20, 10, NA)
mean(score10, na.rm = T)
# 평균
mean(score1)  # [1] 87.2 : 평균값
mean(score2)  # [1] 71.2 : 평균값
mean(score3)  # [1] 71.2 : 평균값
# 중앙값(median:중위수) - 모든 데이터를 크기 순서대로 정렬시킨 후 가운데 있는 값을 의미.
# ex) 100, 100, 54, 50, 52 : 중앙값 -> 54
median(score3) # 54
# ex) 6, 6, 7, 8, 9, 10
num <- c(6, 6, 7, 8, 9, 10)
median(num) # 7.5(=(7+8)/2)
# 분산(Variance) : 편차 값을 제곱해서 마이너스 값을 플러스 값으로 바꾼 후 평균을 구하는 방법.
# ex) ((100-71.2)^2+(100-71.2)^2+(54-71.2)^2+(50-71.2)^2+(52-71.2)^2) / 5 = 554.56
score <- c(100, 100, 54, 50, 52)
mean(score) # 71.2
median(score1) # [1] 86
# 분산(Variance) : score1 <- c(85, 90, 93, 86, 82)
# ((85-86)^2+(90-86)^2+(93-86)^2+(86-86)^2+(82-86)^2)/5 = 16.4
var(score1)    # [1] 18.7
sd(score1)     # [1] 4.32435
# 실습 데이터 셋 가져오기
data <- read.csv("c:/workspaces/Rwork/src/data/descriptive2.csv", header = T)
View(data)
# 데이터 특성 보기
dim(data) # 300 8 - 차원보기
View(data)
length(data) # 8
# 데이터 특성 보기
dim(data) # 300 8 - 차원보기
# 실습 데이터 셋 가져오기
data <- read.csv("c:/workspaces/Rwork/src/data/descriptive2.csv", header = T)
View(data)
length(data) # 8
length(data$survey) # 300
str(data) # 'data.frame':	300 obs. of  8 variables:
str(data$survey)
# 데이터 특성(최소, 최대, 중위수, 평균, 분위수, 노이즈-NA) 제공
summary(data)
# 실습 데이터 셋 가져오기
data <- read.csv("c:/workspaces/Rwork/src/data/descriptive2.csv", header = T)
# 실습 데이터 셋 가져오기
data <- read.csv("c:/workspaces/Rwork/data/descriptive2.csv", header = T)
View(data)
# 데이터 특성 보기
dim(data) # 300 8 - 차원보기
length(data) # 8
length(data$survey) # 300
str(data) # 'data.frame':	300 obs. of  8 variables:
str(data$survey)
# 데이터 특성(최소, 최대, 중위수, 평균, 분위수, 노이즈-NA) 제공
summary(data)
# 2.1 명목척도 기술 통계량
length(data$gender) # 300
summary(data$gender)
table(data$gender) # 각 성별 빈도 수 - outlier(이상치)-> 0, 5
# 이상치 제거
data <- subset(data, data$gender==1 | data$gender==2) # 성별 outlier 제거.
x <- table(data$gender) # 성별에 대한 빈도 수 저장.
x
x11()
barplot(x) # 범주형(명목/서열 척도) 시각화 -> 막대차트
x11()
# 구성비율 계산
prop.table(x) # 비율계산: 0 < x < 1 사이의 값
y <- prop.table(x)
round(y*100, 2) # 백분율 적용(소수점 2자리)
# 2.2 서열척도 기술 통계량
length(data$level) # 297 : 학력수준 - 서열.
summary(data$level) # 명목척도와 함께 의미없음.
table(data$level) # 빈도분석 - 의미있음.
# 학력 수준(level) 변수의 빈도 수 시각화
x1 <- table(data$level) # 각 학력수준에 빈도수 저장.
x11()
barplot(x1)  # 명목/서열 척도 -> 막대차트
# 구성비율 계산
y <- prop.table(x1)
round(y*100,2) # 백분율 적용
# 2.3 등간척도 기술 통계량
# 만족도(survey) 변수 대상 요약 통계량 구하기
survey <- data$survey
survey
summary(survey) # 만족도(5점척도)인 경우 의미 있음 -> 2.605(평균)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's
#1.000   2.000   3.000   2.605   3.000   5.000     112
x1 <- table(survey) # 빈도 수
x1
hist(survey) # 등간척도 시각화
pie(x1)
# 생활비(cost) 변수 대상 요약 통계량 구하기
length(data$cost) # 297
summary(data$cost) # 요약통계량-의미있음(mean)-8.784, 중위수-3.000
plot(data$cost)
# 데이터 정제(이상치 제거)
data <- subset(data, data$cost >= 2 & data$cost <= 10) # 총점기준
data
plot(data$cost)
x <- data$cost
mean(x) # 5.354032
# 평균이 극단치에 영향을 받는 경우 - 중위수(median) 대체
median(x) # 5.4
median(x)
# 생활비(cost) 변수 대상 대표값 구하기
mean(x)
sort(x) # 오름차순
sort(x, decreasing=T) # 내림차순
# 생활비(cost) 변수 대상 사분위수 구하기
quantile(x, 1/4) # 4.6
quantile(x, 2/4) # 5.4
quantile(x, 3/4) # 6.2
quantile(x, 4/4) # 7.9
# 생활비(cost) 변수의 최빈수(빈도 수가 가장 많은 변수) 구하기
length(x) # [1] 248
x.t <- table(x)
x.t
max(x.t) # [1] 18
x.m <- rbind(x.t)
x.m
class(x.m) # [1] "matrix"
str(x.m)
which(x.m[1, ] == 18) # 1행 전체를 대상으로 18값 찾기.
x.df <- as.data.frame(x.m)
class(x.df)
which(x.df[1, ] == 18) # [1] 19(index)
x.df[1, 19] # [1] 18
attributes(x.df) # $names, $class, $row.names
names(x.df[19]) # [1] "5"
attributes(x.df) # $names, $class, $row.names
x.df[1, 19] # [1] 18
attributes(x.df) # $names, $class, $row.names
x.df[1, 19] # [1] 18
attributes(x.df) # $names, $class, $row.names
# 생활비(cost) 변수 대상 산포도 구하기
var(x) # 분산, [1] 1.296826
sd(x)  # 표준편차, [1] 1.138783
# 분산 -> 표준편차
sqrt(var(x))
# 표준편차 -> 분산
sd(x) ** 2
# 생활비(cost) 변수의 빈도분석과 시각화
table(data$cost)
hist(data$cost) # 히스토그램 시각화
plot(data$cost) # 산점도 시각화
# 연속형 변수 범주화
data$cost2[data$cost >= 2 & data$cost < 4] <- 1
data$cost2[data$cost >= 4 & data$cost < 7] <- 2
data$cost2[data$cost >= 7] <- 3
x <- table(data$cost2)
barplot(x)
pie(x)
# 2.5 비대칭도 구하기
install.packages("moments")
library(moments)
cost <- data$cost
cost
# 왜도 - 평균을 중심으로 기울어진 정도.
skewness(cost) # [1] -0.297234
# 첨도 - 표준 정규 분포와 비교하여 얼마나 뽀족한가 측정 지표
kurtosis(cost) # [1] 2.674163
# 기본 히스토그램
hist(cost)
# 히스토그램 확률밀도/표준 정규 분포 곡선
hist(cost, freq = F)
# 확률 밀도 분포 곡선
lines(density(cost), col='blue')
# 표준정규분포 곡선
x <- seq(0, 8, 0.1)
x
curve(dnorm(x, mean(cost), sd(cost)), col='red', add = T)
# attach() / detach() 함수로 기술 통계량 구하기
# - 기존 선언 변수와 컬럼의 이름이 중복시 기존 변수가 global 변수로 잡힘.
search()
attach(iris)
search()
length(Sepal.Width)
summary(Sepal.Width) # 요약 통계량 - 의미있음(mean)
mean(Sepal.Width)
min(Sepal.Width)
max(Sepal.Width)
range(Sepal.Width) # [1] 2.1(min) ~ 7.9(max)
var(Sepal.Width, na.rm = T)
sd <- sd(Sepal.Width, na.rm = T)
sqrt(var(Sepal.Width, na.rm=T))
sort(Sepal.Width) # 오름차순
sort(Sepal.Width, decreasing = T) # 내림차순
detach(iris)
attach(data) # data$cost 접근.
search()
length(cost) # [1] 248
summary(cost) # 요약 통계량 - 의미있음(mean)
mean(cost)
min(cost)
max(cost)
range(cost) # [1] 2.1(min) ~ 7.9(max)
var(cost, na.rm = T)
sd <- sd(cost, na.rm = T)
sqrt(var(cost, na.rm=T))
sort(cost) # 오름차순
sort(cost, decreasing = T) # 내림차순
detach(data)
# 3.1 Hmisc 패키지 이용
install.packages("Hmisc")
library(Hmisc)
# 3.1 Hmisc 패키지 이용
install.packages("Hmisc")
library(Hmisc)
# 전체 변수 대상 기술통계량 제공 - 빈도와 비율 데이터 일괄 수행
describe(data)
# 개별 변수 기술 통계량
describe(data$gender) # 특정 변수(명목) 기술통계량-범주/빈도수/비율 제공.
describe(data$age) # 특정 변수(비율) 기술통계량 - lowest / highest
summary(data$age)
# 3.2 prettyR 패키지 이용
install.packages("prettyR")
# 전체 변수 대상
freq(data) # 각 변수별 : 빈도, 결측치, 백분율, 특징-소수점 제공
# 개별 변수 대상
freq(data$gender) # 빈도, 결측치, 백분율
# 1) 거주지역 변수 리코딩과 비율 계산
data$resident2[data$resident==1] <- "특별시"
data$resident2[data$resident>=2 & data$resident<=4] <- "광역시"
data$resident2[data$resident==5] <- "시구군"
freq(data$resident2)
# 3.2 prettyR 패키지 이용
install.packages("prettyR")
library(prettyR)
# 전체 변수 대상
freq(data) # 각 변수별 : 빈도, 결측치, 백분율, 특징-소수점 제공
# 개별 변수 대상
freq(data$gender) # 빈도, 결측치, 백분율
# 1) 거주지역 변수 리코딩과 비율 계산
data$resident2[data$resident==1] <- "특별시"
data$resident2[data$resident>=2 & data$resident<=4] <- "광역시"
data$resident2[data$resident==5] <- "시구군"
freq(data$resident2)
# 2) 성별 변수 리코딩과 비율 계산
data$gender2[data$gender==1] <- "남자"
data$gender2[data$gender==2] <- "여자"
freq(data$gender2)
# 3) 나이 변수 리코딩과 비율 계산
summary(data$age)
data$age2[data$age <= 45] <- "중년층"
data$age2[data$age >= 46 & data$age <= 59] <- "장년층"
data$age2[data$age >= 60] <- "노년층"
freq(data$age2)
# 4) 학력 수준 변수 리코딩과 비율계산
data$level2[data$level == 1] <- "고졸"
data$level2[data$level == 2] <- "대졸"
data$level2[data$level == 3] <- "대학원졸"
freq(data$level2)
# 5) 합격 여부 변수 리코딩과 비율 계산
data$pass2[data$pass == 1] <- "합격"
data$pass2[data$pass == 2] <- "불합격"
freq(data$pass2)
# 1) 실습 파일 가져오기
data <- read.csv("C:/workspaces/Rwork/data/cleanDescriptive.csv", header = T)
View(data)
# 1) 실습 파일 가져오기
data <- read.csv("C:/workspaces/Rwork/data/cleanDescriptive2.csv", header = T)
View(data)
head(data) # 변수 확인
# 2) 변수 리코딩
x <- data$level2 # 리코딩 변수 이용(학력수준)
y <- data$pass2  # 리코딩 변수 이용(합격/불합격)
x; y  # 부모학력수준(x) -> 자녀대학진학여부(y)
# 3) 데이터프레임 생성
result <- data.frame(Level=x, Pass=y) # 데이터 프레임 생성 - 데이터 묶음.
dim(result) # 차원보기(248,2)
head(result)
View(result)
# 1) 교차 분할표 작성
table(result) # 빈도보기
# 2) 교차분할표 생성을 위한 패키지 설치
install.packages("gmodels")
library(gmodels)
# 3) 패키지를 이용한 교차 분할표 생성
CrossTable(x, y)
# 교차테이블에 카이검정 적용
CrossTable(x, y, chisq = T)
# 1) 실습 파일 가져오기
data <- read.csv("C:/workspaces/Rwork/data/cleanDescriptive.csv", header = T, Encoding("UTF-8"))
# 1) 실습 파일 가져오기
data <- read.csv("C:/workspaces/Rwork/data/cleanDescriptive.csv", header = T, encoding = 'UTF-8')
chisq.test(c(4,6,17,16,8,9))
# 1. 파일 가져오기
data <- read.csv("C:/workspaces/Rwork/src/data/homogenity.csv", header = T)
# 1. 파일 가져오기
data <- read.csv("C:/workspaces/Rwork/data/homogenity.csv", header = T)
View(data)
table(data$method) # 교육방법
table(data$survey) # 만족도
# 전처리 - 결측치/ method와 survey 변수만 서브셋 생성
data <- subset(data, !is.na(survey), c(method, survey))
data
length(data$survey) # 150
# method2 필드 추가
data$method2[data$method == 1] <- "방법1"
data$method2[data$method == 2] <- "방법2"
data$method2[data$method == 3] <- "방법3"
# survey2 필드 추가
data$survey2[data$survey == 1] <- "1. 매우만족"
data$survey2[data$survey == 2] <- "2. 만족"
data$survey2[data$survey == 3] <- "3. 보통"
data$survey2[data$survey == 4] <- "4. 불만족"
data$survey2[data$survey == 5] <- "5. 매우불만족"
# 3. 교차분할표 작성
table(data$method2, data$survey2) # 교차표 생성 -> table(행, 열)
# 4. 교차분할표 생성
CrossTable(data$method2, data$survey2, chisq = T)
# 5. 동질성 검정 - 모수 특성치에 대한 추론 검정
chisq.test(data$method2, data$survey2)
# 단계1. 실습데이터 가져오기
data <- read.csv("C:/workspaces/Rwork/src/data/one_sample.csv", header = T)
# 단계1. 실습데이터 가져오기
data <- read.csv("C:/workspaces/Rwork/data/one_sample.csv", header = T)
head(data)
View(data)
View(data)
# 단계2. 빈도수와 비율 계산
x <- data$survey
x
length(x) # [1] 150
table(x) # 빈도수(0:불만족(14), 1:만족(136))
# 단계3. 패키지 이용 빈도수와 비율계산
install.packages("prettyR")
install.packages("prettyR")
library(prettyR) # freq() 함수 사용
freq(x)
# 이항분포 (불만족율 기준) 비율검정(양측검정)
binom.test(14, 150, p = 0.2) # p-value = 0.0006735
binom.test(14, 150, p = 0.2, alternative = "two.sided", conf.level = 0.95)
# 방향성을 갖는 단측 가설 검정
binom.test(14, 150, p = 0.2, alternative = "greater", conf.level = 0.95) # p-value = 0.9999
binom.test(14, 150, p = 0.2, alternative = "less", conf.level = 0.95) # p-value = 0.0003179
# 이항분포 (불만족율 기준) 비율검정(양측검정)
binom.test(50, 150, p = 0.2) # p-value = 0.0006735
# 이항분포 (불만족율 기준) 비율검정(양측검정)
binom.test(100, 150, p = 0.2) # p-value = 0.0006735
# 이항분포 (불만족율 기준) 비율검정(양측검정)
binom.test(14, 150, p = 0.2) # p-value = 0.0006735
# 이항분포 (불만족율 기준) 비율검정(양측검정)
binom.test(40, 150, p = 0.2) # p-value = 0.0006735
# 이항분포 (불만족율 기준) 비율검정(양측검정)
binom.test(14, 150, p = 0.2) # p-value = 0.0006735
binom.test(14, 150, p = 0.2, alternative = "two.sided", conf.level = 0.95)
# 이항분포 (불만족율 기준) 비율검정(양측검정)
binom.test(14, 150, p = 0.2) # p-value = 0.0006735
binom.test(14, 150, p = 0.2, alternative = "two.sided", conf.level = 0.95)
# 단계6 : 추세요인과 불규칙요인 제거
x11()
plot(tsdata - m$seasonal - m$trend) # 불규칙 요인만 출력.
# 단계4 : 시계열 분해- stl():계절요소, 추세, 잔차 모두 제공.
plot(stl(tsdata, "periodic")) # periodic : 주기
data <- c(45,56,45,43,69,75,58,59,66,64,62,65,
55,49,67,55,71,78,71,65,69,43,70,75,
56,56,65,55,82,85,75,77,77,69,79,89)
# 단계2 : 시계열자료 생성 : 시계열 자료 형식으로 객체 생성
tsdata <- ts(data, start = c(2016, 1), frequency = 12)
tsdata # 2016~2018
# 단계3 : 추세선 확인
par(mfrow=c(1,1))
ts.plot(tsdata) # plot(tsdata)와 동일함.
# 단계4 : 시계열 분해- stl():계절요소, 추세, 잔차 모두 제공.
plot(stl(tsdata, "periodic")) # periodic : 주기
# 단계5 : 시계열 분해와 변동 요인 제거
m <- decompose(tsdata) # decompose()함수 이용 시계열 분해
attributes(m) # 변수 보기
plot(m) # 추세요인, 계절요인, 불규칙 요인이 포함된 그래프.
plot(tsdata - m$seasonal) # 계절요인을 제거한 그래프.
plot(tsdata - m$seasonal - m$trend) # 불규칙 요인만 출력.
# 단계1 : 시계열자료 생성
input <- c(3180,3000,3200,3100,3300,3200,3400,3550,3200,3400,3300,3700)
length(input) # 12
tsdata <- ts(input, start = c(2015, 2), frequency = 12) # Time Series
tsdata
# 단계2 : 자기상관함수 시각화
x11()
acf(na.omit(tsdata), main="자기상관함수", col="red")
# 단계3 : 부분자기상관함수 시각화
pacf(na.omit(tsdata), main="부분자기상관함수", col="red")
# 단계1 : 시계열 자료 생성
input <- c(3180,3000,3200,3100,3300,3200,3400,3550,3200,3400,3300,3700)
# Time Series
tsdata <- ts(input, start = c(2015, 2), frequency = 12)
# 단계2 : 추세선 시각
plot(tsdata, type="l", col="red")
# 단계3 : 자기상관 함수 시각화
acf(na.omit(tsdata), main="자기상관함수", col="red")
# 단계4 : 차분 시각화
plot(diff(tsdata, differences=1))
# 단계1: 시계열  자료 생성
data <- c(45,56,45,43,69,75,58,59,66,64,62,65,
55,49,67,55,71,78,71,65,69,43,70,75,
56,56,65,55,82,85,75,77,77,69,79,89)
length(data) # 36
tsdata <- ts(data, start = c(2016, 1), frequency = 12)
tsdata
# 단계2 : 평활 관련 패키지 설치
install.packages("TTR")
library(TTR)
# 단계3 : 이동평균법으로 평활 및 시각화
par(mfrow=c(2,2))
plot(tsdata, main="원 시계열 자료") # 시계열 자료 시각화
plot(SMA(tsdata, n=1), main="1년 단위 이동평균법으로 평활")
plot(SMA(tsdata, n=2), main="2년 단위 이동평균법으로 평활")
plot(SMA(tsdata, n=3), main="3년 단위 이동평균법으로 평활")
par(mfrow=c(1,1))
# 단계1: 시계열자료 특성분석
# (1) 데이터 준비
input <- c(3180,3000,3200,3100,3300,3200,3400,3550,3200,3400,3300,3700)
# (2) 시계열 객체 생성(12개월:2015년 2월 ~ 2016년 1월)
tsdata <- ts(input, start = c(2015, 2), frequency = 12)
tsdata
# (3) 추세선 시각화(정상성시계열 vs 비정상성시계열)
x11()
plot(tsdata, type="l", col='red')
# 단계2:정상성시계열 변환
par(mfrow=c(1,2))
ts.plot(tsdata)
diff <- diff(tsdata)
plot(diff)
# 단계3: 모형 식별과 추정
install.packages('forecast')
library(forecast)
arima <- auto.arima(tsdata) # 시계열 데이터 이용.
arima
# 단계4: 모형 생성
model <- arima(tsdata, order=c(1,1,0))
model
# 단계5: 모형 진단(모형 타당성 검정)
# (1) 자기상관함수에 의한 모형 진단
tsdiag(model)
# (2) Box-Ljung에 의한 잔차항 모형 진단
Box.test(model$residuals, lag = 1, type = "Ljung")
# (1) 데이터 준비
data <- c(55,56,45,43,69,75,58,59,66,64,62,65,
55,49,67,55,71,78,61,65,69,53,70,75,
56,56,65,55,68,80,65,67,77,69,79,82,
57,55,63,60,68,70,58,65,70,55,65,70)
length(data)# 48
# (2) 시계열자료 생성
tsdata <- ts(data, start=c(2016, 1),frequency=12)
#tsdata <- AirPassengers # 실제 data 적용.
tsdata
head(tsdata)
tail(tsdata)
# (3) 시계열요소분해 시각화
ts_feature <- stl(tsdata, s.window="periodic")
plot(ts_feature)
# 단계2 : 정상성시계열 변환
par(mfrow=c(1,2))
ts.plot(tsdata)
diff <- diff(tsdata)
plot(diff) # 차분 시각화
# 단계3 : 모형 식별과 추정
library(forecast)
ts_model2 <- auto.arima(tsdata)
ts_model2 # ARIMA(0,1,1)(1,1,0)[12] / ARIMA(2,1,1)(0,1,0)[12]
# 단계4 : 모형 생성
model <- arima(tsdata, c(0,1,1), seasonal = list(order = c(1,1,0)))
#model <- arima(tsdata, c(2,1,1), seasonal = list(order = c(0,1,0)))
model
